# Estimator

Estimator の仕組みについて話します。

## Overview

ConfigMap に予測データが保存されると動き始めます。

予測データは CSV 形式で `timestamp`, `yhat`, `yhat_upper`, `yhat_lower` が含まれれることを期待しており、それぞれメトリクスの Unix 時間 (単位: 秒)、予測値、予測値の上ブレ、予測値の下ブレを表しています。

## Receive Data

データを受け取ると CSV でパースし、自身が送信すべきタイムスタンプに調整してソートされます。この調整では EstimationGapMinutes に指定された時間分、タイムスタンプを逆行させます。これは HPA が将来のメトリクスをもとにスケールするようにするための操作です。例えば Gap が 5 分だったとき下記のようなイメージになります。

|timestamp|adjusted timestamp|
|:-:|:-:|
|2020-03-01 12:00:00|2020-03-01 11:55:00|
|2020-03-01 12:05:00|2020-03-01 12:00:00|
|2020-03-01 12:10:00|2020-03-01 12:05:00|
|2020-03-01 12:15:00|2020-03-01 12:10:00|

すでにデータを持っていた場合は古いデータと新しいデータのマージ処理が始まります。一般的に過去の予測結果より最新の予測結果の方が学習データも多く予測のブレも小さいので、古いデータの中で新しいデータに被っている部分がある場合はそこを上書きします。

|timestamp|old data|new data|selected|
|:-:|:-:|:-:|:-:|
|2020-03-01 12:00:00|o|x|old|
|2020-03-01 12:05:00|o|x|old|
|2020-03-01 12:10:00|o|o|new|
|2020-03-01 12:15:00|o|o|new|
|2020-03-01 12:20:00|x|o|new|
|2020-03-01 12:25:00|x|o|new|

## Send Data

データはそのタイムスタンプの時刻が訪れると MetricProvider に送信されます。このとき EstimationMode によっては予測値の調整が行われます。`none` が指定されている場合は `yhat` の値がそのまま送信されます。`adjust` が指定されている場合は、予測したメトリクスの中で実測値が出た直近のデータの予測がどれくらいマッチしていたかによって調整されます。これにより予測が外れたときにも多少対応することが可能です。

前のセクションを踏まえて時系列を下図のように 3 本で考えます。Original Timeline が FittingJob によって生成された時系列で、Estimate Timeline がデータを受け取ったときに EstimateGapMinutes 分タイムスタンプを逆行させた時系列、Provider Timeline が Datadog や Prometheus によって観測されたそのメトリクスの実測値の時系列です。Estimate Timeline が実際にメトリクスを送るタイムラインであり、そのメトリクスの時刻が来るとプロバイダに送信されます。このとき `adjust` モードにしていると Original Timeline 上の過去のデータのうち最も現在時刻に近いデータ (つまり実測値を持つメトリクスのうち直前の予測) が参照され、その実測値を元に送る予測値が調整されます。

![estimator-data](../misc/estimator-data.svg)

例えば前のデータの予測値が `yhat:150, yhat_upper:200, yhat_lower:100` だとしてその実測値が `170` だったとします。実測値を得た時間が時刻 t です。このとき送ろうとしている予測値 (d 時間後の予測) が `yhat:200, yhat_upper:300, yhat_lower:100` とすると、実際に送る予測値は `240` になります。これは `yhat` と `yhat_upper` の差のうちプラス方向にどれだけずれていたかの割合によって計算されます。式で示すと下記のようになります。わかりにくいですが yhat_{t+d} が時刻 t に送る値で、この実測値 y_{t+d} は時刻 t+d に判明します。つまり yhat_{t} は時刻 t-d に送った値であり、時刻 t になってようやく y_{t} が判明しています。

<img src="https://latex.codecogs.com/gif.latex?%5Cfrac%7By_%7Bt%7D-%5Chat%7By%7D_%7Bt%7D%7D%7B%5Chat%7By%7D%5C_upper_%7Bt%7D-%5Chat%7By%7D_%7Bt%7D%7D%20%5Ctimes%20%28%5Chat%7By%7D%5C_upper_%7Bt&plus;d%7D-%5Chat%7By%7D_%7Bt&plus;d%7D%29%20&plus;%20%5Chat%7By%7D_%7Bt&plus;d%7D" />

`yhat_lower` 方向のズレに関しては意図的に無視しています。これは予め負荷を予測するという目的があるため、現在の負荷に対して下方向に合わせてしまうとその目的が薄れてしまうからです (仮に予測値がほぼ正しいなら下方向に合わせても問題ありませんが、負荷の上昇に対応することがメインなのでわざわざそれをする必要はないと思っています)。この調整は `yhat_upper/lower` によって行われるため FittingJob はこれを適切に決める必要があります。デフォルトイメージでは Prophet を使用しており、`upper` は 90%ile までの範囲を、`lower` は 10%ile までの範囲を表しています。調整は `yhat_upper` までの中で行われるため、この幅が広いと予測が大きく外れていた時にもそれに合わせるように調整することができます。一方で、狭いと大きく予測が外れていても `yhat_upper` のところで打ち切りになります。これは予測にマッチしなかった負荷に対応できることと、無駄にリソースを使ってしまうこととのトレードオフです。現在は IHPA はスパイクに対応するように作っていないため `yhat_upper` で止めていますが、良い感じのアイデアがあれば `yhat_upper` 以上の数値に調整することも実装上可能です。

## Datadog specific implementation

Datadog のメトリクス Aggregator には 4 種類 (avg, sum, max, min) あります。Aggregator はメトリクス名の前に `avg:metric_name` のように付与することによって適用することができます。avg はそのメトリクスに対して同一時刻に存在するタグの異なるデータを合計してデータ数で割ったものになります。sum はデータ数で割らない合計を示し、max/min はデータ群の中での max/min を示します。

ここで少し HPA の説明をします。HPA では `要求レプリカ数 = 現在のレプリカ数 * (現在のメトリクス / 要求メトリクス)` の計算式を元にレプリカ数が決定されます ([公式](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details))。「要求メトリクス」は HPA の設定で指定するしきい値のことです。「現在のメトリクス」はメトリクスプロバイダから取得したメトリクスになります。この「現在のメトリクス」に関しては、「要求メトリクス」の指定に `averageValue` または `averageUtilization` を使っていた場合、取得したメトリクス値をレプリカ数で割ったものが「現在のメトリクス」になります。

予測値を HPA に監視させる場合はこの式に気をつける必要があります。各種メトリクスと違い、予測したメトリクスは Estimator から送られるためデータは 1 つしかありません。そのため (当然ですが)、平均値の予測 + HPA の `value`、または合計値の予測 + HPA の `averageValue` のどちらかで監視して上げる必要があります。しかしながらどちらも正解かというとそうではなく、前者は間違いです。論理的にはどちらも正解ですが、FittingJob に学習をさせるという点では合計値について学習させるのが正しいです。なぜなら平均値はそのときの状況によって変動するからです。HPA は負荷に応じてレプリカ数を増減させるため、増えたときにはメトリクスの平均値は小さくなり、減ったときには大きくなります。そのため学習データとしては平均値は不適です。一方で合計値はレプリカ数によって変動しないので適しています (正確にはレプリカ数が増えたときにリクエストが分散されたのに新規に増えた方のメトリクスが取れていなかったりすることもあると思うので常に正しい合計値が出るとは限りませんが概ね適しています)。

上記の考えをもとに学習データの取得には指定されたメトリクスに前述の sum Aggregator を付けたものを使用しています (Datadog Cluster Agent 経由でメトリクスを取得する際は裏で勝手に合計値を計算して返してくれるようなので、HPA 設定上のメトリクス名には Aggregator を付ける必要がありません)。これによって FittingJob は合計値について予測するようになりました。これに加えて Estimator がメトリクス調整をする際も Datadog からメトリクスを取得するため sum Aggregator を付けています。Datadog の実装上 sum Aggregator を付けても送ったホストごとに分割して返してくるため、内部で sum をしています。ちなみにメトリクスを送るときはこの Aggregator は付けないので気をつけます (今はここらへんの実装が少し汚いのでいずれ直したいと思っています)。

以上の話は Datadog を元に書きましたが、これはおそらく Prometheus でも同様の処理が必要になるはずです (なので Aggregator 付与周辺のインターフェースは統一してあります)。
